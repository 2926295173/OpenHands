import json
import os
from collections import defaultdict

import pandas as pd
from datasets import load_dataset

import openhands.agenthub
from evaluation.benchmarks.swe_bench.run_infer import (
    filter_dataset,
    process_instance,
)
from evaluation.utils.shared import (
    EvalMetadata,
    EvalOutput,
    is_fatal_evaluation_error,
    make_metadata,
)
from openhands.core.config import (
    get_llm_config_arg,
    get_parser,
)
from openhands.core.logger import openhands_logger as logger


def run_sequential_evaluation(
    dataset: pd.DataFrame,
    metadata: EvalMetadata,
    output_file: str,
    process_instance_func: callable,
    max_retries: int = 5,
):
    """
    Run evaluation sequentially for instances grouped by repository.

    1. First sort by instance_ids, split them by __ and take the first element as "repo"
    2. Group instance_ids by repo, and then sort them by order
    3. Run the evaluation for each instance in each repo one by one
    4. The first instance id of a repo will start WITHOUT any repo md
       The repo md generated by the first instance will be fed to the evaluation of the second instance
    """
    # Extract repo from instance_id and add as a column
    dataset['repo'] = dataset['instance_id'].apply(lambda x: x.split('__')[0])

    # Group by repo and sort within each group
    repo_groups = defaultdict(list)
    for _, instance in dataset.iterrows():
        repo_groups[instance['repo']].append(instance)

    # Sort instances within each repo group
    for repo in repo_groups:
        repo_groups[repo] = sorted(repo_groups[repo], key=lambda x: x['instance_id'])

    # Process instances sequentially by repo
    total_instances = len(dataset)
    processed_count = 0

    instance_id_to_repo_md = {}
    if os.path.exists(output_file):
        with open(output_file, 'r') as input_fp:
            for line in input_fp:
                result = EvalOutput.model_validate_json(line)
                instance_id_to_repo_md[result.instance_id] = result.test_result[
                    'repo_md'
                ]

    # Open output file for writing results
    output_fp = open(output_file, 'a')

    try:
        # Process each repo group sequentially
        for repo, instances in repo_groups.items():
            logger.info(
                f'Processing repository: {repo} with {len(instances)} instances'
            )

            # Track repo_md across instances in the same repo
            current_repo_md = None

            # Process each instance in the repo sequentially
            for i, instance in enumerate(instances):
                instance_series = pd.Series(instance)

                if instance_series['instance_id'] in instance_id_to_repo_md:
                    # Skip if the instance has already been processed
                    _repo_md = instance_id_to_repo_md[instance_series['instance_id']]
                    if _repo_md is not None and _repo_md.strip() != '':
                        current_repo_md = _repo_md
                    logger.info(
                        f'Skipping instance {instance_series["instance_id"]} because it has already been processed'
                    )
                    processed_count += 1
                    continue

                # Set repo_md from previous instance if available
                if i > 0 and current_repo_md is not None:
                    instance_series['repo_md'] = current_repo_md
                else:
                    instance_series['repo_md'] = None

                # Process the instance
                logger.info(
                    f"Processing instance {instance_series['instance_id']} ({processed_count + 1}/{total_instances})"
                )
                logger.info(
                    f'Repo.md for instance {instance_series["instance_id"]}: {instance_series["repo_md"]}'
                )

                # Try processing with retries
                retry_count = 0
                result = None

                while retry_count < max_retries:
                    try:
                        # Process the instance
                        result: EvalOutput = process_instance_func(
                            instance=instance_series,
                            metadata=metadata,
                            reset_logger=False,
                            runtime_failure_count=retry_count,
                        )

                        # If successful, break out of retry loop
                        if result is not None:
                            break
                    except Exception as e:
                        logger.error(
                            f"Error processing instance {instance_series['instance_id']}: {str(e)}"
                        )
                        if is_fatal_evaluation_error(str(e)):
                            logger.error(f'Fatal error encountered: {str(e)}')
                            break

                        # Increment retry count and try again
                        retry_count += 1
                        logger.info(
                            f"Retrying instance {instance_series['instance_id']} (attempt {retry_count + 1}/{max_retries})"
                        )

                # Write result to output file
                if result is not None:
                    # Extract repo_md from the result for the next instance
                    if result.test_result and 'repo_md' in result.test_result:
                        current_repo_md = result.test_result['repo_md']
                        logger.info(
                            f'Extracted repo_md for next instance in {repo}: {current_repo_md}'
                        )

                    # Write result to output file
                    output_fp.write(result.model_dump_json() + '\n')
                    output_fp.flush()

                processed_count += 1
                logger.info(
                    f"Completed instance {instance_series['instance_id']} ({processed_count}/{total_instances})"
                )

    except KeyboardInterrupt:
        logger.info('Keyboard interrupt received. Cleaning up...')

    finally:
        output_fp.close()

    logger.info('Sequential evaluation completed')


if __name__ == '__main__':
    parser = get_parser()
    parser.add_argument(
        '--dataset',
        type=str,
        default='princeton-nlp/SWE-bench',
        help='data set to evaluate on, either full-test or lite-test',
    )
    parser.add_argument(
        '--split',
        type=str,
        default='test',
        help='split to evaluate on',
    )
    args, _ = parser.parse_known_args()

    # Load dataset from huggingface
    dataset = load_dataset(args.dataset, split=args.split)
    swe_bench_tests = filter_dataset(dataset.to_pandas(), 'instance_id')
    logger.info(
        f'Loaded dataset {args.dataset} with split {args.split}: {len(swe_bench_tests)} tasks'
    )

    # Filter for SWE-Gym verified instances if needed
    if 'SWE-Gym' in args.dataset:
        with open(
            os.path.join(
                os.path.dirname(os.path.abspath(__file__)),
                'split',
                'swegym_verified_instances.json',
            ),
            'r',
        ) as f:
            swegym_verified_instances = json.load(f)
            swe_bench_tests = swe_bench_tests[
                swe_bench_tests['instance_id'].isin(swegym_verified_instances)
            ]
        logger.info(
            f'{len(swe_bench_tests)} tasks left after filtering for SWE-Gym verified instances'
        )

    # Get LLM config
    llm_config = None
    if args.llm_config:
        llm_config = get_llm_config_arg(args.llm_config)
        llm_config.log_completions = True
        # modify_params must be False for evaluation purpose, for reproducibility and accuracy of results
        llm_config.modify_params = False

    if llm_config is None:
        raise ValueError(f'Could not find LLM config: --llm_config {args.llm_config}')

    # Set up metadata
    details = {}
    _agent_cls = openhands.agenthub.Agent.get_cls(args.agent_cls)

    dataset_description = (
        args.dataset.replace('/', '__') + '-' + args.split.replace('/', '__')
    )
    metadata = make_metadata(
        llm_config,
        dataset_description,
        args.agent_cls,
        args.max_iterations,
        args.eval_note,
        args.eval_output_dir,
        details=details,
    )

    output_file = os.path.join(metadata.eval_output_dir, 'output.jsonl')
    print(f'### OUTPUT FILE: {output_file} ###')

    instance_ids = sorted(swe_bench_tests['instance_id'].tolist())
    selected_instance_ids = instance_ids[: args.eval_n_limit]

    instances = swe_bench_tests[
        swe_bench_tests['instance_id'].isin(selected_instance_ids)
    ]
    if len(instances) > 0 and not isinstance(
        instances['PASS_TO_PASS'][instances['PASS_TO_PASS'].index[0]], str
    ):
        for col in ['PASS_TO_PASS', 'FAIL_TO_PASS']:
            instances[col] = instances[col].apply(lambda x: str(x))
    # We will do the filtering in the run_sequential_evaluation function

    logger.info(
        f'Running sequential evaluation for {len(instances)} instances: {instances["instance_id"].tolist()}'
    )
    # Run sequential evaluation
    run_sequential_evaluation(
        instances,
        metadata,
        output_file,
        process_instance,
        max_retries=5,
    )
